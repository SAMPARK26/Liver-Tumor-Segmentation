{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1327578,"sourceType":"datasetVersion","datasetId":767686},{"sourceId":1327590,"sourceType":"datasetVersion","datasetId":769463},{"sourceId":3160712,"sourceType":"datasetVersion","datasetId":1922474}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport glob\n\nimport nibabel as nib\nimport cv2\nimport imageio\nfrom tqdm.notebook import tqdm\nfrom ipywidgets import *\nfrom PIL import Image","metadata":{"_uuid":"89a2929a-3a6a-4ea1-858c-ff9b1a60614c","_cell_guid":"9e7a29d4-1b35-42c5-aa2b-a478fb008e48","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-17T18:16:39.485588Z","iopub.execute_input":"2024-09-17T18:16:39.486018Z","iopub.status.idle":"2024-09-17T18:16:40.759373Z","shell.execute_reply.started":"2024-09-17T18:16:39.485977Z","shell.execute_reply":"2024-09-17T18:16:40.758040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html > /dev/null\n# !pip install --upgrade kornia > /dev/null\n# !pip install allennlp==1.1.0.rc4 > /dev/null","metadata":{"_uuid":"875be407-8e27-486a-9866-7a2e1d457bc6","_cell_guid":"eec92aa0-e556-4280-9838-901cc4402f36","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T04:29:12.580352Z","iopub.execute_input":"2022-02-08T04:29:12.580814Z","iopub.status.idle":"2022-02-08T04:33:03.587954Z","shell.execute_reply.started":"2022-02-08T04:29:12.580778Z","shell.execute_reply":"2022-02-08T04:33:03.586683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install --upgrade fastai > /dev/null\nimport fastai; fastai.__version__","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:34:34.752349Z","iopub.execute_input":"2022-02-09T08:34:34.752869Z","iopub.status.idle":"2022-02-09T08:34:34.763431Z","shell.execute_reply.started":"2022-02-09T08:34:34.752835Z","shell.execute_reply":"2022-02-09T08:34:34.76259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.basics import *\nfrom fastai.vision.all import *\nfrom fastai.data.transforms import *","metadata":{"_uuid":"7f3eb605-82d9-47a9-8696-fc31857c92f4","_cell_guid":"1aa6f319-abe4-4ea1-b94e-31a2ebd4f350","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T08:34:36.622836Z","iopub.execute_input":"2022-02-09T08:34:36.623122Z","iopub.status.idle":"2022-02-09T08:34:39.201474Z","shell.execute_reply.started":"2022-02-09T08:34:36.623091Z","shell.execute_reply":"2022-02-09T08:34:39.200761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a meta file for nii files processing\n\nfile_list = []\nfor dirname, _, filenames in os.walk('../input/liver-tumor-segmentation'):\n    for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        file_list.append((dirname,filename))\n\nfor dirname, _, filenames in os.walk('../input/liver-tumor-segmentation-part-2'):\n    for filename in filenames:\n        file_list.append((dirname,filename))\n\ndf_files = pd.DataFrame(file_list, columns =['dirname', 'filename'])\ndf_files.sort_values(by=['filename'], ascending=True)\n","metadata":{"_uuid":"0355ff93-21b5-434c-aadb-0f7eff3360d2","_cell_guid":"eee2a11c-8260-45bd-baa1-3d9d0d02346a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T06:50:32.72076Z","iopub.execute_input":"2022-02-09T06:50:32.721036Z","iopub.status.idle":"2022-02-09T06:50:32.758302Z","shell.execute_reply.started":"2022-02-09T06:50:32.721004Z","shell.execute_reply":"2022-02-09T06:50:32.757629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map CT scan and label \n\ndf_files[\"mask_dirname\"] = \"\" ; df_files[\"mask_filename\"] = \"\"\n\nfor i in range(131):\n    ct = f\"volume-{i}.nii\"\n    mask = f\"segmentation-{i}.nii\"\n    \n    df_files.loc[df_files['filename'] == ct, 'mask_filename'] = mask\n    df_files.loc[df_files['filename'] == ct, 'mask_dirname'] = \"../input/liver-tumor-segmentation/segmentations\"\n\ndf_files_test= df_files[df_files.mask_filename=='']\n# drop segment rows\ndf_files = df_files[df_files.mask_filename != ''].sort_values(by=['filename']).reset_index(drop=True) \nprint(len(df_files))\ndf_files\n#df_files_test","metadata":{"_uuid":"13d8171f-1996-4ab7-b828-7438986cab82","_cell_guid":"cdae83d9-71f9-4da7-98d1-52524609e673","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T06:50:34.697781Z","iopub.execute_input":"2022-02-09T06:50:34.698575Z","iopub.status.idle":"2022-02-09T06:50:35.088734Z","shell.execute_reply.started":"2022-02-09T06:50:34.698523Z","shell.execute_reply":"2022-02-09T06:50:35.087993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_nii(filepath):\n    '''\n    Reads .nii file and returns pixel array\n    '''\n    ct_scan = nib.load(filepath)\n    array   = ct_scan.get_fdata()\n    array   = np.rot90(np.array(array))\n    return(array)","metadata":{"_uuid":"08f6becb-f7d7-493e-afd7-0e5e29dd09c4","_cell_guid":"63175322-1119-4bb0-9f74-16f2f099bc10","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T08:34:51.621755Z","iopub.execute_input":"2022-02-09T08:34:51.622274Z","iopub.status.idle":"2022-02-09T08:34:51.628562Z","shell.execute_reply.started":"2022-02-09T08:34:51.622224Z","shell.execute_reply":"2022-02-09T08:34:51.627808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read sample\nsample = 3\nsample_ct   = read_nii(df_files.loc[sample,'dirname']+\"/\"+df_files.loc[sample,'filename'])\nsample_mask  = read_nii(df_files.loc[sample,'mask_dirname']+\"/\"+df_files.loc[sample,'mask_filename'])\nprint(sample_ct.shape) \nprint(sample_mask.shape)\nprint(df_files.loc[sample,'dirname']+\"/\"+df_files.loc[sample,'filename'])","metadata":{"_uuid":"6aa7108f-2d7c-4c35-b018-c287a9a3017a","_cell_guid":"a6321875-45f5-48e5-9d66-e9151e9bd514","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T07:32:32.249639Z","iopub.execute_input":"2022-02-09T07:32:32.249915Z","iopub.status.idle":"2022-02-09T07:32:34.752622Z","shell.execute_reply.started":"2022-02-09T07:32:32.249884Z","shell.execute_reply":"2022-02-09T07:32:34.751902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.amin(sample_ct), np.amax(sample_ct))\nprint(np.amin(sample_mask), np.amax(sample_mask))","metadata":{"_uuid":"f860d432-a9ae-4c1d-8560-5c49bb31f232","_cell_guid":"d76e3841-7885-47de-bd03-25cf8831ca1d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T06:50:51.509184Z","iopub.execute_input":"2022-02-09T06:50:51.509445Z","iopub.status.idle":"2022-02-09T06:50:51.975812Z","shell.execute_reply.started":"2022-02-09T06:50:51.509418Z","shell.execute_reply":"2022-02-09T06:50:51.975034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess the nii file \n# Source https://docs.fast.ai/medical.imaging\n\ndicom_windows = types.SimpleNamespace(\n    brain=(80,40),\n    subdural=(254,100),\n    stroke=(8,32),\n    brain_bone=(2800,600),\n    brain_soft=(375,40),\n    lungs=(1500,-600),\n    mediastinum=(350,50),\n    abdomen_soft=(400,50),\n    liver=(150,30),\n    spine_soft=(250,50),\n    spine_bone=(1800,400),\n    custom = (200,60)\n)\n\n@patch\ndef windowed(self:Tensor, w, l):\n    px = self.clone()\n    px_min = l - w//2\n    px_max = l + w//2\n    px[px<px_min] = px_min\n    px[px>px_max] = px_max\n    return (px-px_min) / (px_max-px_min)","metadata":{"_uuid":"064f22d0-57c0-4e96-bf6b-8f0a4df6b4dc","_cell_guid":"58c69412-9509-4a63-a331-b2ab1c61222d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T08:35:21.124538Z","iopub.execute_input":"2022-02-09T08:35:21.124795Z","iopub.status.idle":"2022-02-09T08:35:21.132021Z","shell.execute_reply.started":"2022-02-09T08:35:21.124767Z","shell.execute_reply":"2022-02-09T08:35:21.131297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot a image","metadata":{}},{"cell_type":"code","source":"plt.imshow(tensor(sample_ct[...,50].astype(np.float32)).windowed(*dicom_windows.liver), cmap=plt.cm.bone);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_sample(array_list, color_map = 'nipy_spectral'):\n    '''\n    Plots and a slice with all available annotations\n    '''\n    fig = plt.figure(figsize=(18,15))\n\n    plt.subplot(1,4,1)\n    plt.imshow(array_list[0], cmap='bone')\n    plt.title('Original Image')\n    \n    plt.subplot(1,4,2)\n    plt.imshow(tensor(array_list[0].astype(np.float32)).windowed(*dicom_windows.liver), cmap='bone');\n    plt.title('Windowed Image')\n    \n    plt.subplot(1,4,3)\n    plt.imshow(array_list[1], alpha=0.5, cmap=color_map)\n    plt.title('Mask')\n    \n    plt.subplot(1,4,4)\n    plt.imshow(array_list[0], cmap='bone')\n    plt.imshow(array_list[1], alpha=0.5, cmap=color_map)\n    plt.title('Liver & Mask')\n\n\n    plt.show()","metadata":{"_uuid":"7af56d05-5d7a-4468-877e-7b48fe23b82c","_cell_guid":"082f169a-5ad2-442d-8388-0291c6bcd8a8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T08:35:23.26205Z","iopub.execute_input":"2022-02-09T08:35:23.262601Z","iopub.status.idle":"2022-02-09T08:35:23.270031Z","shell.execute_reply.started":"2022-02-09T08:35:23.262568Z","shell.execute_reply":"2022-02-09T08:35:23.26912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample=410 # this is bscically the slice\nsample_slice = tensor(sample_ct[...,sample].astype(np.float32))\n\nplot_sample([sample_ct[...,sample], sample_mask[...,sample]])","metadata":{"_uuid":"2c17ce56-aab4-4dcd-be7d-607dde75c3fe","_cell_guid":"5c5daccd-23f5-4599-b5db-9a56c85e6e02","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T07:33:07.052427Z","iopub.execute_input":"2022-02-09T07:33:07.053158Z","iopub.status.idle":"2022-02-09T07:33:07.644587Z","shell.execute_reply.started":"2022-02-09T07:33:07.053114Z","shell.execute_reply":"2022-02-09T07:33:07.643984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the mask values\nmask = Image.fromarray(sample_mask[...,sample].astype('uint8'), mode=\"L\")\nprint(mask.shape)\nunique, counts = np.unique(mask, return_counts=True)\nprint( np.array((unique, counts)).T)\nplt.imshow(mask , cmap = 'bone')","metadata":{"_uuid":"62a371e3-b3e3-425e-ac2f-0e0b434cf38d","_cell_guid":"11e5dd59-98cf-4779-86e6-e26bfe2ab286","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing functions\n# Source https://docs.fast.ai/medical.imaging\n\nclass TensorCTScan(TensorImageBW): _show_args = {'cmap':'bone'}\n\n@patch\ndef freqhist_bins(self:Tensor, n_bins=100):\n    \"A function to split the range of pixel values into groups, such that each group has around the same number of pixels\"\n    imsd = self.view(-1).sort()[0]\n    t = torch.cat([tensor([0.001]),\n                   torch.arange(n_bins).float()/n_bins+(1/2/n_bins),\n                   tensor([0.999])])\n    t = (len(imsd)*t).long()\n    return imsd[t].unique()\n    \n@patch\ndef hist_scaled(self:Tensor, brks=None):\n    \"Scales a tensor using `freqhist_bins` to values between 0 and 1\"\n    if self.device.type=='cuda': return self.hist_scaled_pt(brks)\n    if brks is None: brks = self.freqhist_bins()\n    ys = np.linspace(0., 1., len(brks))\n    x = self.numpy().flatten()\n    x = np.interp(x, brks.numpy(), ys)\n    return tensor(x).reshape(self.shape).clamp(0.,1.)\n    \n    \n@patch\ndef to_nchan(x:Tensor, wins, bins=None):\n    res = [x.windowed(*win) for win in wins]\n    if not isinstance(bins,int) or bins!=0: res.append(x.hist_scaled(bins).clamp(0,1))\n    dim = [0,1][x.dim()==3]\n    return TensorCTScan(torch.stack(res, dim=dim))\n\n@patch\ndef save_jpg(x:(Tensor), path, wins, bins=None, quality=90):\n    fn = Path(path).with_suffix('.jpg')\n    x = (x.to_nchan(wins, bins)*255).byte()\n    im = Image.fromarray(x.permute(1,2,0).numpy(), mode=['RGB','CMYK'][x.shape[0]==4])\n    im.save(fn, quality=quality)","metadata":{"_uuid":"5af29678-4fe3-4374-9edb-d2d67aae1669","_cell_guid":"938eb50f-8407-4028-9fe8-b8c2911b018f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T08:36:04.811481Z","iopub.execute_input":"2022-02-09T08:36:04.811896Z","iopub.status.idle":"2022-02-09T08:36:04.824042Z","shell.execute_reply.started":"2022-02-09T08:36:04.81186Z","shell.execute_reply":"2022-02-09T08:36:04.823399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test plot","metadata":{}},{"cell_type":"code","source":"_,axs=subplots(1,1)\n\nsample_slice.save_jpg('test.jpg', [dicom_windows.liver,dicom_windows.custom])\nshow_image(Image.open('test.jpg'), ax=axs[0])\nshow_image(Image.open('test.png'), ax=axs[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_files=df_files[100:131]\ndf_files=df_files[100:111]\ndf_files\n\n\n# df_files=df_files[0:100]\n# df_files\n\n# test = df_files[101:130]","metadata":{"_uuid":"5e50c1c0-b49a-4683-a967-c145b51e5ffd","_cell_guid":"2cc3d730-69c8-45cc-bd68-5abea22c7cde","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T13:40:12.314494Z","iopub.execute_input":"2022-02-08T13:40:12.314753Z","iopub.status.idle":"2022-02-08T13:40:12.327179Z","shell.execute_reply.started":"2022-02-08T13:40:12.314724Z","shell.execute_reply":"2022-02-08T13:40:12.326316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make custom JPG files for Unet training\n# Total number of 131 nii files contains 67072 slices \n\nGENERATE_JPG_FILES = True   # warning: generation takes ~ 1h\nslice_sum=0\nif (GENERATE_JPG_FILES):\n    \n    path = Path(\".\")\n\n    os.makedirs('train_images',exist_ok=True)\n    os.makedirs('train_masks',exist_ok=True)\n\n    for ii in tqdm(range(100+0,100+len(df_files))): # take 1/3 nii files for training\n        curr_ct        = read_nii(df_files.loc[ii,'dirname']+\"/\"+df_files.loc[ii,'filename'])\n        curr_mask      = read_nii(df_files.loc[ii,'mask_dirname']+\"/\"+df_files.loc[ii,'mask_filename'])\n        curr_file_name = str(df_files.loc[ii,'filename']).split('.')[0]\n        curr_dim       = curr_ct.shape[2] # 512, 512, curr_dim\n        slice_sum = slice_sum+curr_dim\n        \n        for curr_slice in range(0,curr_dim,1): # export every 2nd slice for training\n            data = tensor(curr_ct[...,curr_slice].astype(np.float32))\n            mask = Image.fromarray(curr_mask[...,curr_slice].astype('uint8'), mode=\"L\")\n            data.save_jpg(f\"train_images/{curr_file_name}_slice_{curr_slice}.jpg\", [dicom_windows.liver,dicom_windows.custom])\n            mask.save(f\"train_masks/{curr_file_name}_slice_{curr_slice}_mask.png\")\n            \nelse:\n    \n    path = Path(\"../input/liver-segmentation-with-fastai-v2\") # read jpg from saved kernel output\nprint(slice_sum)","metadata":{"_uuid":"0ffd9acf-665c-43a4-992a-2befea9405ae","_cell_guid":"f1f2a0ff-b70a-4aad-9088-3dcb29ee2cdb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T13:40:20.557787Z","iopub.execute_input":"2022-02-08T13:40:20.558074Z","iopub.status.idle":"2022-02-08T13:42:16.810819Z","shell.execute_reply.started":"2022-02-08T13:40:20.558021Z","shell.execute_reply":"2022-02-08T13:42:16.810012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !zip -q -r train_masks.zip \"./train_masks\"\n# !zip -q -r train_images.zip \"./train_images\"","metadata":{"_uuid":"db0aab2d-a601-4928-9f6c-716217b21b63","_cell_guid":"38145c94-e909-4c63-a6a0-4044ccf7f2b0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T10:49:28.407607Z","iopub.execute_input":"2022-02-08T10:49:28.408191Z","iopub.status.idle":"2022-02-08T10:50:34.768075Z","shell.execute_reply.started":"2022-02-08T10:49:28.408149Z","shell.execute_reply":"2022-02-08T10:50:34.767065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -rf ./train_images\n# !rm -rf ./train_masks","metadata":{"_uuid":"cbc4c825-b7bc-4e46-b0af-9b3879204de3","_cell_guid":"c204bef9-4c88-44c7-b2c7-63764c48cb1a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL TRAINING","metadata":{"_uuid":"4b8cf549-c5bc-4e4a-8ffd-fe3bc20b9eaa","_cell_guid":"79fe9f34-f4ab-4897-86ea-d9d078523d50","trusted":true}},{"cell_type":"code","source":"bs = 16\nim_size = 128\n\ncodes = np.array([\"background\",\"liver\",\"tumor\"])\n    \ndef get_x(fname:Path): \n    return fname\n\ndef label_func(x): \n    return path/'train_masks'/f'{x.stem}_mask.png'\n\ntfms = [IntToFloatTensor(),Normalize()]\n\ndb = DataBlock(blocks=(ImageBlock(),MaskBlock(codes)),  #codes = {\"Backround\": 0,\"Liver\": 1,\"Tumor\": 2}\n               batch_tfms=tfms,\n               splitter=RandomSplitter(),\n               item_tfms=[Resize(im_size)],\n               get_items=get_image_files,\n               get_y=label_func\n              )\n\n# ../output/kaggle/working/train_images.zip\n# ds = db.datasets(source=path/'train_images.zip')\nds = db.datasets(source='./train_images')\nprint(len(ds))\nprint(ds)","metadata":{"_uuid":"aaf75b9c-4149-4764-a107-ad8437c5c0d3","_cell_guid":"fd2b01e2-7b08-4e2b-91cb-9c420accc2e0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T13:47:25.677861Z","iopub.execute_input":"2022-02-08T13:47:25.678389Z","iopub.status.idle":"2022-02-08T13:47:25.779605Z","shell.execute_reply.started":"2022-02-08T13:47:25.678348Z","shell.execute_reply":"2022-02-08T13:47:25.778728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx=20\nimgs = [ds[idx][0],ds[idx][1]]\nfig,axs = plt.subplots(1, 2)\nfor i,ax in enumerate(axs.flatten()):\n    ax.axis('off')\n    ax.imshow(imgs[i]) #, cmap='gray'","metadata":{"_uuid":"6b7f7623-d1fe-484f-962b-767319074491","_cell_guid":"1fccd2c3-0441-4e5f-b365-8f65a04440d7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T13:44:59.137179Z","iopub.execute_input":"2022-02-08T13:44:59.137618Z","iopub.status.idle":"2022-02-08T13:44:59.37565Z","shell.execute_reply.started":"2022-02-08T13:44:59.137578Z","shell.execute_reply":"2022-02-08T13:44:59.374874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique, counts = np.unique(array(ds[idx][1]), return_counts=True)\n\nprint( np.array((unique, counts)).T)","metadata":{"_uuid":"13906c21-1325-44e5-bfe1-9c051ca729b5","_cell_guid":"cab0fc62-f9c1-4e1b-b1f1-36890a982856","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T13:47:43.97216Z","iopub.execute_input":"2022-02-08T13:47:43.972465Z","iopub.status.idle":"2022-02-08T13:47:43.999971Z","shell.execute_reply.started":"2022-02-08T13:47:43.97243Z","shell.execute_reply":"2022-02-08T13:47:43.999252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = db.dataloaders(path/'train_images',bs = bs) #, num_workers=0\ndls.show_batch()","metadata":{"_uuid":"172af566-20eb-4221-9055-9cfe31d61219","_cell_guid":"6f6fd9e5-4ae6-4597-a0e2-4df089f91ad2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T13:48:24.55358Z","iopub.execute_input":"2022-02-08T13:48:24.554246Z","iopub.status.idle":"2022-02-08T13:48:28.638473Z","shell.execute_reply.started":"2022-02-08T13:48:24.554205Z","shell.execute_reply":"2022-02-08T13:48:28.636783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def foreground_acc(inp, targ, bkg_idx=0, axis=1):  # exclude a background from metric\n    \"Computes non-background accuracy for multiclass segmentation\"\n    targ = targ.squeeze(1)\n    mask = targ != bkg_idx\n    return (inp.argmax(dim=axis)[mask]==targ[mask]).float().mean() \n\ndef cust_foreground_acc(inp, targ):  # # include a background into the metric\n    return foreground_acc(inp=inp, targ=targ, bkg_idx=3, axis=1) # 3 is a dummy value to include the background which is 0","metadata":{"_uuid":"4b6a5826-4dcf-4e0d-9840-f88e9b786586","_cell_guid":"bd31834f-7ba0-40b8-9c2f-b3e4f6ccffb9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T10:58:06.934668Z","iopub.execute_input":"2022-02-08T10:58:06.935491Z","iopub.status.idle":"2022-02-08T10:58:06.941786Z","shell.execute_reply.started":"2022-02-08T10:58:06.935444Z","shell.execute_reply":"2022-02-08T10:58:06.940972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = unet_learner(dls, resnet34, loss_func=CrossEntropyLossFlat(axis=1), metrics=[foreground_acc, cust_foreground_acc])","metadata":{"_uuid":"3917dba5-0051-4f6e-b9a4-7752090cbc4a","_cell_guid":"60d76559-41b4-46f8-a683-12567c82f33c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T10:58:08.768711Z","iopub.execute_input":"2022-02-08T10:58:08.769214Z","iopub.status.idle":"2022-02-08T10:58:13.134987Z","shell.execute_reply.started":"2022-02-08T10:58:08.769176Z","shell.execute_reply":"2022-02-08T10:58:13.13414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"_uuid":"3343194e-7b8e-4b81-b1c7-f7124a479b70","_cell_guid":"f945cc1f-8b48-432b-bc54-827a172a539d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T10:58:15.072222Z","iopub.execute_input":"2022-02-08T10:58:15.072928Z","iopub.status.idle":"2022-02-08T10:58:37.140389Z","shell.execute_reply.started":"2022-02-08T10:58:15.072892Z","shell.execute_reply":"2022-02-08T10:58:37.139652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fine_tune(5, wd=0.1, cbs=SaveModelCallback() )","metadata":{"_uuid":"3c16f47a-5750-4670-a43a-1ab15423bcb7","_cell_guid":"02f8dd15-41ca-476e-b9b7-ca246a7bc0bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T10:59:33.211619Z","iopub.execute_input":"2022-02-08T10:59:33.211935Z","iopub.status.idle":"2022-02-08T11:16:58.376783Z","shell.execute_reply.started":"2022-02-08T10:59:33.2119Z","shell.execute_reply":"2022-02-08T11:16:58.375946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.show_results()","metadata":{"_uuid":"e7cc782c-af7f-4f3b-a49f-4106626df420","_cell_guid":"5b5abc8c-32c2-46b0-93a1-881b0142440f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T11:17:20.454502Z","iopub.execute_input":"2022-02-08T11:17:20.455005Z","iopub.status.idle":"2022-02-08T11:17:21.676503Z","shell.execute_reply.started":"2022-02-08T11:17:20.454963Z","shell.execute_reply":"2022-02-08T11:17:21.675226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nlearn.export(path/f'Liver_segmentation')","metadata":{"_uuid":"c744e917-7b8f-4dc4-809d-6d2d43866a01","_cell_guid":"4284c310-2b14-4e31-8ad0-c1b75c74e1f0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T11:17:59.34006Z","iopub.execute_input":"2022-02-08T11:17:59.34066Z","iopub.status.idle":"2022-02-08T11:17:59.783478Z","shell.execute_reply.started":"2022-02-08T11:17:59.340619Z","shell.execute_reply":"2022-02-08T11:17:59.782697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ndel learn\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"_uuid":"ab84b523-f589-4e59-8503-033ed76f46f9","_cell_guid":"5427a10a-fc11-4aa1-a1fc-9c41d8699d16","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-08T11:18:13.045259Z","iopub.execute_input":"2022-02-08T11:18:13.046003Z","iopub.status.idle":"2022-02-08T11:18:13.376265Z","shell.execute_reply.started":"2022-02-08T11:18:13.045964Z","shell.execute_reply":"2022-02-08T11:18:13.375397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING MODEL","metadata":{"_uuid":"ae994f2f-9d3a-41de-9246-be785f8ddd74","_cell_guid":"10bc85e7-9ec7-4dbd-a5f2-24440eea1df6","trusted":true}},{"cell_type":"markdown","source":"### Run this cell to do the required imports","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport glob\n\nimport nibabel as nib\nimport cv2\nimport imageio\nfrom tqdm.notebook import tqdm\nfrom ipywidgets import *\nfrom PIL import Image\n\n\nimport fastai; \nprint(fastai.__version__)\nfrom fastai.basics import *\nfrom fastai.vision.all import *\nfrom fastai.data.transforms import *","metadata":{"execution":{"iopub.status.busy":"2022-02-09T09:14:51.351666Z","iopub.execute_input":"2022-02-09T09:14:51.351915Z","iopub.status.idle":"2022-02-09T09:14:51.358412Z","shell.execute_reply.started":"2022-02-09T09:14:51.351889Z","shell.execute_reply":"2022-02-09T09:14:51.357682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Run this cell to make sure the data files are loaded into a variable called as `df_files` (it has to a pandas dataframe) This cell also defines the function required for nii file format conversion","metadata":{}},{"cell_type":"code","source":"# Create a meta file for nii files processing\n\nfile_list = []\nfor dirname, _, filenames in os.walk('../input/liver-tumor-segmentation'):\n    for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        file_list.append((dirname,filename)) \n\nfor dirname, _, filenames in os.walk('../input/liver-tumor-segmentation-part-2'):\n    for filename in filenames:\n        file_list.append((dirname,filename))\n\ndf_files = pd.DataFrame(file_list, columns =['dirname', 'filename'])\n\n\n# Map CT scan and label \ndf_files[\"mask_dirname\"] = \"\" ; df_files[\"mask_filename\"] = \"\"\n\nfor i in range(131):\n    ct = f\"volume-{i}.nii\"\n    mask = f\"segmentation-{i}.nii\"\n    \n    df_files.loc[df_files['filename'] == ct, 'mask_filename'] = mask\n    df_files.loc[df_files['filename'] == ct, 'mask_dirname'] = \"../input/liver-tumor-segmentation/segmentations\"\n\ndf_files_test= df_files[df_files.mask_filename=='']\n# drop segment rows\ndf_files = df_files[df_files.mask_filename != ''].sort_values(by=['filename']).reset_index(drop=True) \nprint(len(df_files))\n\n# function used to read nii files and convert into a numpy array\ndef read_nii(filepath):\n    '''\n    Reads .nii file and returns pixel array\n    '''\n    ct_scan = nib.load(filepath)\n    array   = ct_scan.get_fdata()\n    array   = np.rot90(np.array(array))\n    return(array)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T10:05:58.305884Z","iopub.execute_input":"2022-02-09T10:05:58.306168Z","iopub.status.idle":"2022-02-09T10:05:58.468334Z","shell.execute_reply.started":"2022-02-09T10:05:58.306138Z","shell.execute_reply":"2022-02-09T10:05:58.46751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Use the below cell to split the dataset into test and train. This assumes that the same split was used for training the model and the model has not seen the test datas","metadata":{}},{"cell_type":"code","source":"# df_files=df_files[100:131]\n# df_files\n\n# first 20 data points\ndf_file = df_files[0:20]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T12:02:09.680137Z","iopub.execute_input":"2022-02-08T12:02:09.680712Z","iopub.status.idle":"2022-02-08T12:02:09.700273Z","shell.execute_reply.started":"2022-02-08T12:02:09.680673Z","shell.execute_reply":"2022-02-08T12:02:09.699533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Below cell imports the trained model and also defines a few variables and functions required for the tetsing.\n**Make sure you define the path as per your system**","metadata":{}},{"cell_type":"markdown","source":"Definning things required for loading the model","metadata":{}},{"cell_type":"code","source":"# Load saved model\nbs = 16\nim_size = 128\n\n# the labels used for the classes\n# When predicting the model predicts it in terms of indices (ie 0 --> background, 1 --> liver ...)\ncodes = np.array([\"background\",\"liver\",\"tumor\"])\n\n# the default pathb\npath = './'\n\ndef get_x(fname:Path): \n    return fname\n\ndef label_func(x): \n    return path/'train_masks'/f'{x.stem}_mask.png'\n\ndef foreground_acc(inp, targ, bkg_idx=0, axis=1):  # exclude a background from metric\n    \"Computes non-background accuracy for multiclass segmentation\"\n    targ = targ.squeeze(1)\n    mask = targ != bkg_idx\n    return (inp.argmax(dim=axis)[mask]==targ[mask]).float().mean() \n\ndef cust_foreground_acc(inp, targ):  # # include a background into the metric\n    return foreground_acc(inp=inp, targ=targ, bkg_idx=3, axis=1) # 3 is a dummy value to include the background which is 0","metadata":{"_uuid":"804ec6d7-dfa9-4f0d-b53e-df2676413c88","_cell_guid":"7d8ca0a8-8afe-442f-8d34-63aec9fec404","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T10:06:03.709584Z","iopub.execute_input":"2022-02-09T10:06:03.710125Z","iopub.status.idle":"2022-02-09T10:06:03.717698Z","shell.execute_reply.started":"2022-02-09T10:06:03.710085Z","shell.execute_reply":"2022-02-09T10:06:03.716863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the model","metadata":{}},{"cell_type":"code","source":"# loading the tensor flow model\ntfms = [Resize(im_size), IntToFloatTensor(),Normalize()]\nlearn0 = load_learner('../input/trained-model/Liver_segmentation',cpu=False )\nlearn0.dls.transform = tfms","metadata":{"execution":{"iopub.status.busy":"2022-02-09T10:06:06.819868Z","iopub.execute_input":"2022-02-09T10:06:06.820113Z","iopub.status.idle":"2022-02-09T10:06:08.739428Z","shell.execute_reply.started":"2022-02-09T10:06:06.820085Z","shell.execute_reply":"2022-02-09T10:06:08.73872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Below function helps to convert the nii file to the format used by the model. So that prediction is possible","metadata":{}},{"cell_type":"code","source":"def nii_tfm(fn,wins): \n\n    test_nii  = read_nii(fn)\n    curr_dim  = test_nii.shape[2] # 512, 512, curr_dim\n    slices = []\n    \n#     for curr_slice in range(curr_dim):\n#         data = tensor(test_nii[...,curr_slice].astype(np.float32))\n#         data = (data.to_nchan(wins)*255).byte()\n#         slices.append(TensorImage(data))\n        \n#     return slices\n    data = tensor(test_nii[...,450].astype(np.float32))\n    data = (data.to_nchan(wins)*255).byte()\n    slices.append(TensorImage(data))\n#     data = tensor(test_nii[...,351].astype(np.float32))\n#     data = (data.to_nchan(wins)*255).byte()\n#     slices.append(TensorImage(data))\n    print(slices)\n    return slices","metadata":{"_uuid":"5bb7a4ae-0f06-40f9-9441-dc6749e4fc41","_cell_guid":"4049b167-6fc6-4275-924e-b79b229ac2a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T10:30:08.687691Z","iopub.execute_input":"2022-02-09T10:30:08.688224Z","iopub.status.idle":"2022-02-09T10:30:08.693743Z","shell.execute_reply.started":"2022-02-09T10:30:08.688187Z","shell.execute_reply":"2022-02-09T10:30:08.693058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Selecting the test number and the slice number for which the prediction is to be done","metadata":{}},{"cell_type":"code","source":"# test number\ntst = 3\n\n# slice number\ntest_slice_idx = 450\n\n\ntest_nii   = read_nii(df_files.loc[tst,'dirname']+\"/\"+df_files.loc[tst,'filename'])\ntest_mask  = read_nii(df_files.loc[tst,'mask_dirname']+\"/\"+df_files.loc[tst,'mask_filename'])\nprint(test_nii.shape)\n\nsample_slice = tensor(test_nii[...,test_slice_idx].astype(np.float32))\n\nplot_sample([test_nii[...,test_slice_idx], test_mask[...,test_slice_idx]])","metadata":{"_uuid":"7c92f492-4d28-4be4-a17c-7805398c8520","_cell_guid":"f98fa719-9003-4011-b843-7deebf7262cc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T10:32:24.453788Z","iopub.execute_input":"2022-02-09T10:32:24.454064Z","iopub.status.idle":"2022-02-09T10:32:26.812097Z","shell.execute_reply.started":"2022-02-09T10:32:24.454026Z","shell.execute_reply":"2022-02-09T10:32:26.811496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare a nii test file for prediction \n\ntest_files = nii_tfm(df_files.loc[tst,'dirname']+\"/\"+df_files.loc[tst,'filename'],[dicom_windows.liver, dicom_windows.custom])\nprint(\"Number of test slices: \",len(test_files))","metadata":{"_uuid":"181e9412-aa08-4a77-8152-6b0023f7ff9a","_cell_guid":"fba83f50-afb8-4f17-91b2-66213334be0b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T10:32:29.848966Z","iopub.execute_input":"2022-02-09T10:32:29.849424Z","iopub.status.idle":"2022-02-09T10:32:30.781226Z","shell.execute_reply.started":"2022-02-09T10:32:29.849388Z","shell.execute_reply":"2022-02-09T10:32:30.780356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check an input for a test file\nshow_image(test_files[0])","metadata":{"_uuid":"bf12d9e5-f3bb-4b7b-89f1-e2be1ba225f8","_cell_guid":"6637e1f9-33e6-4698-b290-a274563ed6c5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T10:32:33.3061Z","iopub.execute_input":"2022-02-09T10:32:33.306946Z","iopub.status.idle":"2022-02-09T10:32:33.508683Z","shell.execute_reply.started":"2022-02-09T10:32:33.306896Z","shell.execute_reply":"2022-02-09T10:32:33.508066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions for a Test file\n\ntest_dl = learn0.dls.test_dl(test_files)\npreds,y = learn0.get_preds(dl=test_dl)\n\npredicted_mask = np.argmax(preds, axis=1)\nprint(type(predicted_mask))\nplt.imshow(predicted_mask[0])","metadata":{"_uuid":"d4ca3c2c-b36e-4601-a446-83c52afd1715","_cell_guid":"d082da21-2489-4164-ad46-8decaf2ae855","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T10:32:36.488343Z","iopub.execute_input":"2022-02-09T10:32:36.488919Z","iopub.status.idle":"2022-02-09T10:32:36.888919Z","shell.execute_reply.started":"2022-02-09T10:32:36.48888Z","shell.execute_reply":"2022-02-09T10:32:36.888077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=np.array(predicted_mask[0])\n\nunique, counts = np.unique(a, return_counts=True)\nprint( np.array((unique, counts)).T)\n\nnp.amin(a),np.amax(a),","metadata":{"_uuid":"cd7e678c-2067-4e1b-9833-bd66f1092f64","_cell_guid":"f6e5b3da-079b-4ef6-8c5d-24447ed56d39","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T10:32:38.957331Z","iopub.execute_input":"2022-02-09T10:32:38.957905Z","iopub.status.idle":"2022-02-09T10:32:38.968293Z","shell.execute_reply.started":"2022-02-09T10:32:38.957867Z","shell.execute_reply":"2022-02-09T10:32:38.967316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting predictions done on multiple images.\n\n../input/trained-model","metadata":{}},{"cell_type":"code","source":"\ndef nii_tfm_selctive(fn,wins,curr_slice): \n    slices = []\n    test_nii  = read_nii(fn)\n    data = tensor(test_nii[...,curr_slice].astype(np.float32))\n    data = (data.to_nchan(wins)*255).byte()\n    slices.append(TensorImage(data))\n    return slices\n\ndef check(img):\n    cnt,h = cv2.findContours(img,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n    if len(cnt) > 0:\n        return 1\n    else:\n        return 0\n    \n\nnums = [3,4,5]\nvol_names = ['volume-100.nii','volume-102.nii','volume-102.nii']\nseg_names = ['segmentation-100.nii','segmentation-102.nii','segmentation-102.nii']\n\n\nconf_matrix =  np.zeros((2,2),dtype = int)\n\n\nfor nums,vol_names,seg_names in zip(nums,vol_names,seg_names):\n    curr_mask      = read_nii(df_files.loc[nums,'mask_dirname']+\"/\"+df_files.loc[nums,'mask_filename'])\n    for curr_slice in tqdm(range(250,550,10)): # export every 10th slice for testing\n        \n        # for prediction\n        test_file = nii_tfm_selctive(df_files.loc[nums,'dirname']+\"/\"+df_files.loc[nums,'filename'],[dicom_windows.liver, dicom_windows.custom],curr_slice)\n        test_dl = learn0.dls.test_dl(test_file)\n        preds, y = learn0.get_preds(dl=test_dl)\n        \n        predicted_mask = np.argmax(preds, axis=1)# getting the predicted mask\n        plt.imshow(predicted_mask[0])\n        a=np.array(predicted_mask[0])\n        \n        tumor_p = False\n        \n        print('curr slice:',curr_slice)\n        \n        unique = np.unique(a)\n        print(\"predicted\",unique)\n        if 0 in unique:\n            back_p = True\n        else:\n            back_p = False\n        if 1 in unique:\n            liver_p = True\n        else:\n            liver_p = False\n        if 2 in unique:\n            tumor_p = True\n        else:\n            tumor_p = False\n        \n\n        # for getiing the actual mask values\n        mask = Image.fromarray(curr_mask[...,curr_slice].astype('uint8'), mode=\"L\")\n        tumor_t = False\n        \n        unique = np.unique(mask)\n        print(\"actual:\",unique)\n        if 0 in unique:\n            back_t = True\n        else:\n            back_t = False\n        if 1 in unique:\n            liver_t = True\n        else:\n            liver_t = False\n        if 2 in unique:\n            tumor_t = True\n        else:\n            tumor_t = False\n            \n        # populating the conf_matrix\n        if tumor_p == True and tumor_t == True:\n            conf_matrix[0,0] += 1\n        if tumor_p == False and tumor_t == False:\n            conf_matrix[1,1] += 1\n        if tumor_p == False and tumor_t == True:\n            conf_matrix[1,0] += 1\n        if tumor_p == True and tumor_t == False:\n            conf_matrix[0,1] += 1\n            \n        \nprint(conf_matrix)\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Confusion Matrix","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nax = sns.heatmap(conf_matrix, annot=True, cmap='Blues')\n\nax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\nax.set_xlabel('\\nActual Values')\nax.set_ylabel('Predicted Values ');\n\n## Ticket labels - List must be in alphabetical order\nax.xaxis.set_ticklabels(['True','False'])\nax.yaxis.set_ticklabels(['True','False'])\n\n## Display the visualization of the Confusion Matrix.\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T10:37:04.799367Z","iopub.execute_input":"2022-02-09T10:37:04.799643Z","iopub.status.idle":"2022-02-09T10:37:05.002628Z","shell.execute_reply.started":"2022-02-09T10:37:04.799613Z","shell.execute_reply":"2022-02-09T10:37:05.001922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting the `.nii` files to jpg and converting the corresponding mask.\n### This will later be used for validating the model","metadata":{}},{"cell_type":"code","source":"nums = [3,4,5]\nvol_names = ['volume-100.nii','volume-102.nii','volume-102.nii']\nseg_names = ['segmentation-100.nii','segmentation-102.nii','segmentation-102.nii']\n\ntotal_slice = 0\n# print(df_files.loc[nums,'dirname']+\"/\"+df_files.loc[nums,'filename'])\n\nfor nums,vol_names,seg_names in zip(nums,vol_names,seg_names):\n    curr_ct        = read_nii(df_files.loc[nums,'dirname']+\"/\"+df_files.loc[nums,'filename'])\n    curr_mask      = read_nii(df_files.loc[nums,'mask_dirname']+\"/\"+df_files.loc[nums,'mask_filename'])\n    curr_file_name = str(df_files.loc[nums,'filename']).split('.')[0]\n    curr_dim       = curr_ct.shape[2] # 512, 512, curr_dim\n\n    for curr_slice in tqdm(range(250,550,10)): # export every 10th slice for testing\n        data = tensor(curr_ct[...,curr_slice].astype(np.float32))\n        mask = Image.fromarray(curr_mask[...,curr_slice].astype('uint8'), mode=\"L\")\n        data.save_jpg(f\"images/{curr_file_name}_slice_{curr_slice}.jpg\", [dicom_windows.liver,dicom_windows.custom])\n        mask.save(f\"mask/{curr_file_name}_slice_{curr_slice}_mask.png\")\n        total_slice = total_slice+1\n        \nprint(total_slice)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:52:46.135406Z","iopub.execute_input":"2022-02-09T06:52:46.135994Z","iopub.status.idle":"2022-02-09T06:52:56.794821Z","shell.execute_reply.started":"2022-02-09T06:52:46.135957Z","shell.execute_reply":"2022-02-09T06:52:56.793995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using opencv to find if the mask of an scan image is showing any of the features (background,liver and tumor). The data is being saved into a csv files. This can later be used to validate the model by comparing the model prediction with the csv files.","metadata":{}},{"cell_type":"code","source":"import cv2 \nimport numpy as np\nimport os\n\n\ndef check(img):\n    cnt,h = cv2.findContours(img,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n    if len(cnt) > 0:\n        return 1\n    else:\n        return 0\n\nf = open('true_values.csv',mode = 'w')\n\nfile_iterator = os.scandir('./mask')\nfor i in file_iterator:\n    \n    img = cv2.imread(\"./mask/\"+i.name,-1)\n    print(img.shape)\n    \n    # if only the background is visible\n    if np.count_nonzero(img)==0:\n        f.write('0,0,0\\n')\n    else:\n        f.write('0,')\n        # for liver\n        img_liver = np.where(img == 1, 255, img)\n        img_liver = np.where(img_liver == 2, 0, img)\n        ret = check(img_liver)\n        if ret:\n            f.write('1,')\n        else:\n            f.write('0,')\n\n        # for tumor\n        img_tumor = np.where(img == 2, 255, img)\n        img_tumor = np.where(img_tumor == 1, 0, img)\n        ret = check(img_tumor)\n        if ret:\n            f.write('1')\n        else:\n            f.write('0')\n\n        f.write('\\n')\n\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}